{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79852d62-5030-41dd-b175-fa48f0ec5cf1",
   "metadata": {},
   "source": [
    "## boo!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f59c8-fb48-4195-bfcf-9fd29a304ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T20:24:06.278267Z",
     "iopub.status.busy": "2022-10-13T20:24:06.277541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output’: File exists\n",
      "mkdir: cannot create directory ‘input’: File exists\n",
      "mkdir: cannot create directory ‘tempoutput’: File exists\n",
      "mkdir: cannot create directory ‘tempoutput2’: File exists\n",
      "mkdir: cannot create directory ‘tempoutput3’: File exists\n",
      "rm: cannot remove 'input/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'output': Is a directory\n",
      "rm: cannot remove 'tempoutput': Is a directory\n",
      "rm: cannot remove 'tempoutput2': Is a directory\n",
      "rm: cannot remove 'tempoutput3': Is a directory\n",
      "rm: cannot remove 'tempoutput2/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'tempoutput3/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'tempoutput/.ipynb_checkpoints': No such file or directory\n",
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-wfasliqp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-wfasliqp\n",
      "  Resolved https://github.com/openai/whisper.git to commit d18e9ea5dd2ca57c697e8e55f9e654f06ede25d0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from whisper==1.0) (1.23.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from whisper==1.0) (1.12.0+cu116)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from whisper==1.0) (4.64.0)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.9/dist-packages (from whisper==1.0) (4.20.1)\n",
      "Collecting ffmpeg-python==0.2.0\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.18.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisper==1.0) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->whisper==1.0) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.1.0)\n",
      "Building wheels for collected packages: whisper\n",
      "  Building wheel for whisper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175109 sha256=a54ec02bc3c94c0921da8883e51dbac5d8beb1228d39cb70363d488eb1a8045d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zhfcwqho/wheels/fe/03/29/e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
      "Successfully built whisper\n",
      "Installing collected packages: more-itertools, ffmpeg-python, whisper\n",
      "Successfully installed ffmpeg-python-0.2.0 more-itertools-8.14.0 whisper-1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ". .......... ..........  715K\n",
      "    50K .......... .                                           8.08M=0.07s\n",
      "\n",
      "2022-10-13 20:25:24 (862 KB/s) - ‘./input/2022-09-06_15-30-40.mp3’ saved [62964]\n",
      "\n",
      "--2022-10-13 20:25:24--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-34-28.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:5\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-34-28.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  568K\n",
      "    50K .......... .......... .......... .......... .......... 1.61M\n",
      "   100K .......... .......... .......... .......... .......... 3.75M\n",
      "   150K .......... ......                                      2.91M=0.1s\n",
      "\n",
      "2022-10-13 20:25:29 (1.19 MB/s) - ‘./input/2022-09-06_15-34-28.mp3’ saved [170532]\n",
      "\n",
      "--2022-10-13 20:25:29--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-36-51.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1201::6e:1\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-36-51.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.57M\n",
      "    50K .......... .......... ..........                       3.97M=0.04s\n",
      "\n",
      "2022-10-13 20:25:30 (2.04 MB/s) - ‘./input/2022-09-06_15-36-51.mp3’ saved [82296]\n",
      "\n",
      "--2022-10-13 20:25:30--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-39-00.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:2\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-39-00.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.25M\n",
      "    50K .......... .......... ..........                       4.69M=0.05s\n",
      "\n",
      "2022-10-13 20:25:30 (1.73 MB/s) - ‘./input/2022-09-06_15-39-00.mp3’ saved [82728]\n",
      "\n",
      "--2022-10-13 20:25:30--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-44-32.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:5\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-44-32.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.20M\n",
      "    50K .......... .......... .......... .......... .......... 2.70M\n",
      "   100K .......... ..                                          8.56M=0.06s\n",
      "\n",
      "2022-10-13 20:25:30 (1.83 MB/s) - ‘./input/2022-09-06_15-44-32.mp3’ saved [115560]\n",
      "\n",
      "--2022-10-13 20:25:30--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-45-15.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:5\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-45-15.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.25M\n",
      "    50K .......... .......... .......... .....                 3.41M=0.05s\n",
      "\n",
      "2022-10-13 20:25:30 (1.70 MB/s) - ‘./input/2022-09-06_15-45-15.mp3’ saved [87480]\n",
      "\n",
      "--2022-10-13 20:25:30--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-47-00.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:2\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-47-00.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.55M\n",
      "    50K .......... .......... .......... .......... .......... 1.74M\n",
      "   100K .......... .......... .......... .......... .......... 1.56M\n",
      "   150K .......... .......... .......... .......... .......... 2.19M\n",
      "   200K .......... .......... .......... .......... .......... 3.81M\n",
      "   250K .......... .......... .....                            3.70M=0.1s\n",
      "\n",
      "2022-10-13 20:25:30 (2.03 MB/s) - ‘./input/2022-09-06_15-47-00.mp3’ saved [282204]\n",
      "\n",
      "--2022-10-13 20:25:30--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-48-38.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1201::6e:1\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-48-38.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 2.72M\n",
      "    50K .......... .......... .......... .......               2.68M=0.03s\n",
      "\n",
      "2022-10-13 20:25:31 (2.70 MB/s) - ‘./input/2022-09-06_15-48-38.mp3’ saved [89208]\n",
      "\n",
      "--2022-10-13 20:25:31--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-49-08.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1201::6e:1\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-49-08.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  495K\n",
      "    50K .......... .......... .......... .......... .......... 1.54M\n",
      "   100K .......... .....                                       6.37M=0.1s\n",
      "\n",
      "2022-10-13 20:25:36 (855 KB/s) - ‘./input/2022-09-06_15-49-08.mp3’ saved [118260]\n",
      "\n",
      "--2022-10-13 20:25:36--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-52-34.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:2\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-52-34.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.21M\n",
      "    50K .......... .......... .......... .......... .........  3.25M=0.06s\n",
      "\n",
      "2022-10-13 20:25:36 (1.76 MB/s) - ‘./input/2022-09-06_15-52-34.mp3’ saved [101952]\n",
      "\n",
      "--2022-10-13 20:25:36--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_15-53-18.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:5\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_15-53-18.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 2.17M\n",
      "    50K .......... .......... ..                               3.07M=0.03s\n",
      "\n",
      "2022-10-13 20:25:36 (2.39 MB/s) - ‘./input/2022-09-06_15-53-18.mp3’ saved [74088]\n",
      "\n",
      "--2022-10-13 20:25:36--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_16-02-30.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:5\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_16-02-30.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.20M\n",
      "    50K .......... .......... .......... .......... .......... 3.02M\n",
      "   100K ......                                                 12241G=0.06s\n",
      "\n",
      "2022-10-13 20:25:36 (1.82 MB/s) - ‘./input/2022-09-06_16-02-30.mp3’ saved [108972]\n",
      "\n",
      "--2022-10-13 20:25:36--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_16-03-24.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1202::6e:2\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_16-03-24.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 1.30M\n",
      "    50K .......... .......... .......... .......... .......... 2.64M\n",
      "   100K .......... .......... .......... .......... .......... 7.09M\n",
      "   150K ..                                                     5588G=0.06s\n",
      "\n",
      "2022-10-13 20:25:37 (2.38 MB/s) - ‘./input/2022-09-06_16-03-24.mp3’ saved [156600]\n",
      "\n",
      "--2022-10-13 20:25:37--  https://huexrb.ngrok.io/2021-selected/wendy/2022-09-06_16-05-34.mp3\n",
      "Resolving huexrb.ngrok.io (huexrb.ngrok.io)... 3.134.39.220, 2600:1f16:d83:1201::6e:1\n",
      "Connecting to huexrb.ngrok.io (huexrb.ngrok.io)|3.134.39.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./input/2022-09-06_16-05-34.mp3’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 2.35M\n",
      "    50K .......... .......... .......... .                     3.26M=0.03s\n",
      "\n",
      "2022-10-13 20:25:37 (2.63 MB/s) - ‘./input/2022-09-06_16-05-34.mp3’ saved [83160]\n",
      "\n",
      "rm: cannot remove 'input/.ipynb_checkpoints': No such file or directory\n",
      "100%|█████████████████████████████████████| 2.87G/2.87G [01:16<00:00, 40.2MiB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py\", line 304, in cli\n",
      "    model = load_model(model_name, device=device, download_root=model_dir)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/whisper/__init__.py\", line 113, in load_model\n",
      "    return model.to(device)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.80 GiB total capacity; 6.93 GiB already allocated; 6.31 MiB free; 7.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!mkdir output\n",
    "!mkdir input\n",
    "!mkdir tempoutput\n",
    "!mkdir tempoutput2\n",
    "!mkdir tempoutput3\n",
    "!rm *.zip\n",
    "!rm tempoutput/*\n",
    "!rm output/*\n",
    "!rm input/*\n",
    "!rm -r 'input/.ipynb_checkpoints'\n",
    "!rm *output*\n",
    "!rm tempoutput3/*\n",
    "!rm tempoutput2/*\n",
    "!rm -r 'tempoutput2/.ipynb_checkpoints'\n",
    "!rm -r 'tempoutput3/.ipynb_checkpoints'\n",
    "!rm -r 'tempoutput/.ipynb_checkpoints'\n",
    "!pip install git+https://github.com/openai/whisper.git \n",
    "!pip install requests\n",
    "!pip install pysrt\n",
    "!sudo apt update && sudo apt install ffmpeg\n",
    "\n",
    "import pysrt\n",
    "from datetime import timedelta\n",
    "import string\n",
    "import requests\n",
    "import os\n",
    "\n",
    "mylistw = open(\"input.txt\").read().splitlines()\n",
    "for i in mylistw:\n",
    "    os.system(\n",
    "        \"wget https://huexrb.ngrok.io/2021-selected/wendy/\" + i + \".mp3 -P ./input/\"\n",
    "    )\n",
    "    \n",
    "os.system(\"rm -r 'input/.ipynb_checkpoints'\")\n",
    "\n",
    "dir = os.scandir(\"input\")\n",
    "entries = [it.name for it in dir]\n",
    "for i in entries:\n",
    "    x = i\n",
    "    os.system(\"whisper 'input/\" + x + \"' --language English --model large -o output\")\n",
    "    \n",
    "# def get_seconds(time_str):\n",
    "#     hh, mm, ss = time_str.split(\":\")\n",
    "#     return float(hh) * 3600 + float(mm) * 60 + float(ss)\n",
    "\n",
    "\n",
    "# def get_end_time(line_num):\n",
    "#     return str(get_seconds(str(subs[line_num].end.to_time())))\n",
    "\n",
    "\n",
    "# def get_start_time(line_num):\n",
    "#     return str(get_seconds(str(subs[line_num].start.to_time())))\n",
    "\n",
    "\n",
    "# def get_text(line_num):\n",
    "#     x = subs[line_num].text\n",
    "#     x = x.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "#     return x.upper()\n",
    "\n",
    "\n",
    "# def whole_line(line_num):\n",
    "#     return (\n",
    "#         get_start_time(line_num)\n",
    "#         + \" \"\n",
    "#         + get_end_time(line_num)\n",
    "#         + \" \"\n",
    "#         + get_text(line_num)\n",
    "#     )\n",
    "\n",
    "\n",
    "# mylist = open(\"input.txt\").read().splitlines()\n",
    "# for z in mylist:\n",
    "#     subs = pysrt.open(\"./output/\" + z + \".mp3.srt\")\n",
    "#     data = \"\"\n",
    "#     length = len(subs)\n",
    "#     for i in range(length):\n",
    "#         data = data + whole_line(i) + \"\\n\"\n",
    "#     headers = {\n",
    "#         \"Host\": \"huexrb.ngrok.io\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "#         \"Accept\": \"/\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "#         \"Origin\": \"https://huexrb.ngrok.io\",\n",
    "#         \"Referer\": \"https://huexrb.ngrok.io/tagger?user=%27wendy%27\",\n",
    "#         \"Sec-Fetch-Dest\": \"empty\",\n",
    "#         \"Sec-Fetch-Mode\": \"cors\",\n",
    "#         \"Sec-Fetch-Site\": \"same-origin\",\n",
    "#     }\n",
    "#     data1 = \"2021-selected/wendy/\" + z + \"-wendy.txt\\n\" + data\n",
    "#     response = requests.post(\"https://huexrb.ngrok.io/\", headers=headers, data=data1)\n",
    "#     print(response.text)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab123da-bf42-406d-8307-1fb732e81b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = os.scandir(\"input\")\n",
    "entries = [it.name for it in dir]\n",
    "for i in entries:\n",
    "    x = i\n",
    "    os.system(\"whisper 'input/\" + x + \"' --language English --model large -o output\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ed5c9-561a-46f0-a48d-3b245ca83905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"cp output/*.txt  tempoutput\")\n",
    "\n",
    "directory = os.scandir(\"tempoutput\")\n",
    "entries = [it.name for it in directory]\n",
    "for i in entries:\n",
    "    x = i\n",
    "    os.system(\"touch tempoutput2/\" + x +\" && tr -d '[:punct:]' <tempoutput/\" + x + \" >tempoutput2/\" + x)\n",
    "    \n",
    "    \n",
    "directory2 = os.scandir(\"tempoutput2\")\n",
    "entriesx = [it.name for it in directory2]\n",
    "entries2 = [\"tempoutput2/\" + e for e in entriesx]\n",
    "entries3 = [\"tempoutput3/\" + e for e in entriesx]\n",
    "for x,z in zip(entries2, entries3):\n",
    "    with open(x, 'r') as inp:\n",
    "        y = inp.read().upper()\n",
    "    with open(z, 'a') as out:\n",
    "        out.write(y)    \n",
    "\n",
    "os.system(\"zip -r -j $(TZ=Asia/Kolkata date '+%Y-%m-%d-%H')_output.zip tempoutput3/*\")\n",
    "\n",
    "\n",
    "# os.system(\"rm *output*\")\n",
    "# os.system(\"touch $(date '+%Y-%m-%d-%H:%M')_output.txt\")\n",
    "outputs = open(\"input.txt\").read().splitlines()\n",
    "for i in outputs:\n",
    "    x = i + \".mp3.txt\"\n",
    "    os.system(\"echo \" + i + \">> $(TZ=Asia/Kolkata date '+%Y-%m-%d-%H')_output.txt\")\n",
    "    os.system('echo \"\" >> $(TZ=Asia/Kolkata date \"+%Y-%m-%d-%H\")_output.txt')\n",
    "    os.system(\"cat ./tempoutput3/\" + x + \">> $(TZ=Asia/Kolkata date '+%Y-%m-%d-%H')_output.txt\")\n",
    "    os.system('echo \"\" >> $(TZ=Asia/Kolkata date \"+%Y-%m-%d-%H\")_output.txt')\n",
    "    os.system('echo \"\" >> $(TZ=Asia/Kolkata date \"+%Y-%m-%d-%H\")_output.txt')\n",
    "    os.system('echo \"\" >> $(TZ=Asia/Kolkata date \"+%Y-%m-%d-%H\")_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12917e-58d2-4560-99f0-922eb73d5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install pysrt\n",
    "\n",
    "\n",
    "import pysrt\n",
    "from datetime import timedelta\n",
    "import string\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def get_seconds(time_str):\n",
    "    hh, mm, ss = time_str.split(\":\")\n",
    "    return float(hh) * 3600 + float(mm) * 60 + float(ss)\n",
    "\n",
    "\n",
    "def get_end_time(line_num):\n",
    "    return str(get_seconds(str(subs[line_num].end.to_time())))\n",
    "\n",
    "\n",
    "def get_start_time(line_num):\n",
    "    return str(get_seconds(str(subs[line_num].start.to_time())))\n",
    "\n",
    "\n",
    "def get_text(line_num):\n",
    "    x = subs[line_num].text\n",
    "    x = x.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return x.upper()\n",
    "\n",
    "\n",
    "def whole_line(line_num):\n",
    "    return (\n",
    "        get_start_time(line_num)\n",
    "        + \" \"\n",
    "        + get_end_time(line_num)\n",
    "        + \" \"\n",
    "        + get_text(line_num)\n",
    "    )\n",
    "\n",
    "\n",
    "mylist = open(\"input.txt\").read().splitlines()\n",
    "for z in mylist:\n",
    "    subs = pysrt.open(\"./output/\" + z + \".mp3.srt\")\n",
    "    data = \"\"\n",
    "    length = len(subs)\n",
    "    for i in range(length):\n",
    "        data = data + whole_line(i) + \"\\n\"\n",
    "    headers = {\n",
    "        \"Host\": \"huexrb.ngrok.io\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "        \"Accept\": \"/\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"Origin\": \"https://huexrb.ngrok.io\",\n",
    "        \"Referer\": \"https://huexrb.ngrok.io/tagger?user=%27wendy%27\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    }\n",
    "    data1 = \"2021-selected/wendy/\" + z + \"-wendy.txt\\n\" + data\n",
    "    response = requests.post(\"https://huexrb.ngrok.io/\", headers=headers, data=data1)\n",
    "    print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
